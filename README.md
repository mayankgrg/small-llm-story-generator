# Pico LLM

Pico LLM is a lightweight, modular, and highly extensible framework for building small-scale language models using custom neural architectures. The project supports CNN-based, MLP-based, and hybrid token encoders, enabling experimentation with computationally efficient NLP models.

---

## ğŸš€ Features

**â€¢ Modular Architecture** â€” Switch easily between K-gram CNN, K-gram MLP, or other encoders.

**â€¢ Custom Token Processors** â€” Implement windowing, stacking, positional embeddings, or byte-level processing.

**â€¢ Flexible Training Pipeline** â€” Custom training loops for debugging and rapid prototyping.

**â€¢ Long-Range Context Support** â€” Optional dilated CNN layers and layer normalization.

**â€¢ Compact and Deployable** â€” Designed to run on limited hardware.

---

## ğŸ“ Project Structure (High-Level Architecture)

```
pico-llm/
â”‚
â”œâ”€â”€ pico-llm.py               # Main training script
â”œâ”€â”€ KgramCNN.py               # CNN-based k-gram encoder
â”œâ”€â”€ kgramMLP.py               # MLP-based k-gram encoder
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ dataset_loader.py     # Loads and tokenizes text
â”‚   â””â”€â”€ sample_text.txt       # Sample dataset
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ transformer_block.py  # Optional transformer experiments
â”‚   â””â”€â”€ utils.py              # Shared functions (loss, initialization, etc.)
â”‚
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ trainer.py            # Epoch loop, batching, evaluation
â”‚   â””â”€â”€ metrics.py            # Perplexity, accuracy, loss curves
â”‚
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ config.json           # Model hyperparameters
â”‚   â””â”€â”€ results/              # Logs and checkpoints
â”‚
â””â”€â”€ README.md                 # Project documentation
```

---

## ğŸ”§ Installation

```
git clone <repo-url>
cd pico-llm
pip install -r requirements.txt
```

---

## ğŸ§  Model Architectures

### 1. **K-gram CNN Encoder (KgramCNN.py)**

* Multi-channel 1D CNN
* Layer Normalization (optional)
* Dilated convolutions for long-range context
* Max-pooling or attention-like aggregation

### 2. **K-gram MLP Encoder (kgramMLP.py)**

* Fully-connected layers over k-gram windows
* Fast for small vocabularies and tiny models
* Dropout/LN supported

### 3. **Main LLM Wrapper (pico-llm.py)**

Handles:

* Tokenization
* Dataset slicing
* Model loading
* Training loop
* Validation logging

---

## ğŸ“Š Training

```
python pico-llm.py --model kgram_cnn \
                   --epochs 20 \
                   --lr 3e-4 \
                   --context 128
```

You can switch between encoder types:

```
--model kgram_mlp
--model kgram_cnn
```

---

## ğŸ“¦ Output

The framework generates:

* `.pt` model checkpoints
* training logs
* perplexity curves
* generated text samples

---

## ğŸ§© Example Architecture Diagram

```
                +---------------------------+
                |       Dataset Loader      |
                +--------------+------------+
                               |
                               v
                    +----------+----------+
                    |   Token Processor   |
                    +----------+----------+
                               |
                    +----------v----------+
                    |   K-gram Builder   |
                    +----------+----------+
                               |
            +------------------v------------------+
            |       Encoder Module (Choose)       |
            |  - KgramCNN                         |
            |  - KgramMLP                         |
            +------------------+------------------+
                               |
                      +--------v--------+
                      |   LLM Head      |
                      +--------+--------+
                               |
                        +------v------+
                        |   Trainer   |
                        +-------------+
```

---

## ğŸ› ï¸ Future Enhancements

* Add Rotary Positional Embeddings
* Add lightweight attention block
* Add Byte-level tokenizer
* Add benchmarking suite

---

## ğŸ¤ Contributing

Pull requests are welcome. Please open an issue for major changes.

---

## ğŸ“œ License

MIT License.
