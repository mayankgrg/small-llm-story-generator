# Pico LLM

Pico LLM is a lightweight, modular, and highly extensible framework for building small-scale language models using custom neural architectures. The project supports CNN-based, MLP-based, and hybrid token encoders, enabling experimentation with computationally efficient NLP models.

---

## ğŸš€ Features

**â€¢ Modular Architecture** â€” Switch easily between K-gram CNN, K-gram MLP, or other encoders.

**â€¢ Custom Token Processors** â€” Implement windowing, stacking, positional embeddings, or byte-level processing.

**â€¢ Flexible Training Pipeline** â€” Custom training loops for debugging and rapid prototyping.

**â€¢ Long-Range Context Support** â€” Optional dilated CNN layers and layer normalization.

**â€¢ Compact and Deployable** â€” Designed to run on limited hardware.

---

## ğŸ“ Project Structure (High-Level Architecture)

```
pico-llm/
â”‚
â”œâ”€â”€ pico-llm.py               # Main training script
â”œâ”€â”€ KgramCNN.py               # CNN-based k-gram encoder
â”œâ”€â”€ kgramMLP.py               # MLP-based k-gram encoder
â”‚â”€â”€ LSTM.py                   # LSTM model
|â”€â”€ Transformer.py            # Transformer model
â”‚
â””â”€â”€ README.md                 # Project documentation
```

---

## ğŸ”§ Installation

```
git clone <repo-url>
cd pico-llm
pip install -r requirements.txt
```

---

## ğŸ§  Model Architectures

### 1. **K-gram CNN Encoder (KgramCNN.py)**

* Multi-channel 1D CNN
* Layer Normalization (optional)
* Dilated convolutions for long-range context
* Max-pooling or attention-like aggregation

### 2. **K-gram MLP Encoder (kgramMLP.py)**

* Fully-connected layers over k-gram windows
* Fast for small vocabularies and tiny models
* Dropout/LN supported

### 3. **Main LLM Wrapper (pico-llm.py)**

Handles:

* Tokenization
* Dataset slicing
* Model loading
* Training loop
* Validation logging

---

## ğŸ“Š Training

```
python pico-llm.py --model kgram_cnn \
                   --epochs 20 \
                   --lr 3e-4 \
                   --context 128
```

You can switch between encoder types:

```
--model kgram_mlp
--model kgram_cnn
```

---

## ğŸ“¦ Output

The framework generates:

* `.pt` model checkpoints
* training logs
* perplexity curves
* generated text samples


---

## ğŸ› ï¸ Future Enhancements

* Add Rotary Positional Embeddings
* Add lightweight attention block
* Add Byte-level tokenizer
* Add benchmarking suite

---

## ğŸ¤ Contributing

Pull requests are welcome. Please open an issue for major changes.
